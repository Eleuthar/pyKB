OVERVIEW
``````````
1. Define async function (coroutine)
2. loop = asyncio.get_event_loop()
3. task_obj = loop.create_task(some_coroutine(*arg, **kwarg))
4. loop.run_until_complete(task_obj)
	4.1 task_obj.cancel() -> CancelledError 
5. loop.close()



Coroutines 
```````````
* use async libs for full async effect
* prime via 'next()', allow execution to advance to 'yield' expression
* send values to coroutine via 'send()'
* consume values sent to them via '(yield)'
* can suspend and resume execution
* close it via 'close()'


# turn generator into coroutine
def grep(pattern):
	print('Looking for ", pattern)
	while True:
		line = (yield)
		if pattern in line:
			print(line)

grepper = grep('patterns')
next(grepper)
	\\ Output: "Looking for patterns"
grepper.send('ZZZZZ')
grepper.send("XXXXX")
grepper.close()

# define coroutine up to Py 3.5
@asyncio.coroutine
def ex_coro():
	yield from fun()

# as of Py 3.5
async def ex_coroutine():
	await fun()



Scheduling calls
````````````````
* import functools to force asyncio to receive kwarg for stopping the loop

# call infinite loop
loop.run_forever()

loop.call_soon(functools.partial(function_name, *arg, **kwarg))
loop.call_soon_threadsafe(function_name, *arg)
loop.call_later(<int>, function_name, *arg)

# grab loop's time instead of computer time
current_time = loop.time()
loop.call_at(current_time + 300, function_name, loop)



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
"""
1. Make aiohttp.ClientSession to pass the coroutine for each url within tasks
	1.1 same effect as using asyncio.get_event_loop().create_task(async_function(arg))
2. Make timer using async_timeout context manager that will exit on timeout
3. Make response object via session.get()
4. Read response content as aiohttp.StreamReader object to allow chunk & write to disk
	4.1 aiofiles can be used to async write to disk
5. Release response StreamReader - don't rely on context manager
"""

import aiohttp
import asyncio
import async_timeout
import os


async def download_coroutine(session, url):
	with async_timeout.timeout(10):
		async with session.get(url) as response:
			filename = os.path.basename(url)
			with open(filename, 'wb') as f_handle:
				while True:
					chunk = await response.content.read(1024)
					if not chunk:
						break
					f_handle.write(chunk)
			return await response.release()


async def main(loop):
	urlz = [
		"http://www.irs.gov/pub/irs-pdf/f1040.pdf",
		"http://www.irs.gov/pub/irs-pdf/f1040a.pdf",
		"http://www.irs.gov/pub/irs-pdf/f1040ez.pdf",
		"http://www.irs.gov/pub/irs-pdf/f1040es.pdf",
		"http://www.irs.gov/pub/irs-pdf/f1040sb.pdf"
	]

	async with aiohttp.ClientSession(loop=loop) as session:
		tasks = [download_coroutine(session, url) for url in urlz]
		await asyncio.gather(*tasks)


if __name__ == '__main__':
	loop = asyncio.get_event_loop()
	loop.run_until_complete(main(loop))


