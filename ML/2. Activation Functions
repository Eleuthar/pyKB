Activation Functions: Functions like ReLU, Sigmoid, and Tanh introduce non-linearity.
